{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fStN8jasNnd1K2coW3S2mD9VWjXqIPWT","timestamp":1683564410696}],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["pip install easy_vqa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5__T13-84UC","executionInfo":{"status":"ok","timestamp":1683576045313,"user_tz":240,"elapsed":3782,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}},"outputId":"f9768cf3-cafa-46fa-dc1b-c23109963938"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: easy_vqa in /usr/local/lib/python3.10/dist-packages (1.0)\n"]}]},{"cell_type":"code","execution_count":108,"metadata":{"id":"qXs-VGOhuysK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683576054621,"user_tz":240,"elapsed":7,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}},"outputId":"60f435c1-850e-49c7-dbeb-12ffc77dee28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision.transforms import transforms\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","class VQAModel(nn.Module):\n","    def __init__(self, num_answers):\n","        super(VQAModel, self).__init__()\n","        \n","        # image feature extraction with pre-trained ResNet-18 model\n","        self.resnet = models.resnet18(pretrained=True).to(device)\n","        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1]).to(device) # remove last fully connected layer\n","        \n","        # quesetion feature extraction using bag of words (BoW) model\n","        self.bow = CountVectorizer()\n","        \n","        # combine image and question features using a fully connected layer\n","        self.fc = nn.Sequential(\n","            nn.Linear(512 + 512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_answers)\n","        ).to(device)\n","\n","    def forward(self, image, question):\n","        # Extract image features\n","        image_features = self.resnet(image)\n","        image_features = image_features.view(image_features.size(0), -1)\n","\n","        # combine image & question features, pad with 0s\n","        extra = torch.zeros((len(question), 512 - len(question[0]))).to(device)\n","        combined_features = torch.cat((image_features, question), dim=1).to(device)\n","        combined_features = torch.cat((combined_features, extra), dim = 1).to(device)\n","\n","        # output logits for each answer choice\n","        return self.fc(combined_features)"]},{"cell_type":"code","source":["from easy_vqa import get_train_questions, get_train_image_paths, get_test_questions, get_test_image_paths, get_answers\n","from PIL import Image\n","\n","def load_dataset():\n","    # load questions, image paths, and answers\n","    train_qs, train_answers, train_image_ids = get_train_questions()\n","    test_qs, test_answers, test_image_ids = get_test_questions()\n","    train_image_paths = get_train_image_paths()\n","    test_image_paths = get_test_image_paths()\n","    answers = get_answers()\n","\n","    # preprocess the images\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    train_images = [transform(Image.open(train_image_paths[id]).convert('RGB')) for id in train_image_ids[:200]]\n","    test_images = [transform(Image.open(test_image_paths[id]).convert('RGB')) for id in test_image_ids[:200]]\n","\n","    return train_images[:200], train_qs[:200], train_answers[:200], test_images[:200], test_qs[:200], test_answers[:200], answers"],"metadata":{"id":"GwnTSzyvveGX","executionInfo":{"status":"ok","timestamp":1683575927143,"user_tz":240,"elapsed":9,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class VQADataset(Dataset):\n","    def __init__(self, images, questions, answers):\n","        self.images = images\n","        self.questions = questions\n","        \n","        self.answers = answers\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        return self.images[idx], self.questions[idx], self.answers[idx]\n","\n","def create_dataloader(images, questions, answers, batch_size=64):\n","    dataset = CustomDataset(images, questions, answers)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, questions, answers):\n","        self.images = images\n","        self.questions = questions\n","        self.answers = answers\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        return self.images[idx], self.questions[idx], self.answers[idx]"],"metadata":{"id":"cbUoEpUXvtzU","executionInfo":{"status":"ok","timestamp":1683575931455,"user_tz":240,"elapsed":19,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","def train_vqa_model(model, train_dataloader, epochs=10, learning_rate=0.001):\n","    model.train() # set model to training mode\n","    # loss function & optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for images, questions, answers in train_dataloader:\n","\n","            images = images.to(device)\n","            # fit_transform\n","            questions = model.bow.transform([q for q in questions])\n","            questions = torch.tensor(questions.todense()).float().to(device)\n","\n","            # convert answers to numerical labels\n","            int_labels = []\n","            for ans in answers:\n","                int_labels.append(label_dict[ans])\n","            int_labels = torch.tensor(int_labels).to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images, questions)\n","            loss = criterion(outputs, int_labels).to(device)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_dataloader)}')\n","\n","\n","train_images, train_qs, train_answers, test_images, test_qs, test_answers, answers = load_dataset()\n","\n","label_dict = {}\n","i = 0\n","# assign index to each of 13 possible answers\n","for ans in answers:\n","    label_dict[ans] = i\n","    i += 1\n","\n","# initialize the VQA model with 13 answer choices (Easy-VQA dataset)\n","model = VQAModel(num_answers=13)\n","\n","# fit the BoW model with the vocabulary of the questions in the dataset\n","model.bow.fit(train_qs)\n","\n","train_dataloader = create_dataloader(train_images, train_qs, train_answers)\n","train_vqa_model(model, train_dataloader, 100)"],"metadata":{"id":"DHC_oliAx4c9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683575998826,"user_tz":240,"elapsed":61464,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}},"outputId":"1dee8d6b-cc1f-42c6-ebd3-b68e8eda82e4"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 2.0141628682613373\n","Epoch 2/100, Loss: 1.724893033504486\n","Epoch 3/100, Loss: 1.5875495076179504\n","Epoch 4/100, Loss: 1.4964751601219177\n","Epoch 5/100, Loss: 1.4175163507461548\n","Epoch 6/100, Loss: 1.3211841881275177\n","Epoch 7/100, Loss: 1.3830206990242004\n","Epoch 8/100, Loss: 1.414231687784195\n","Epoch 9/100, Loss: 1.361294448375702\n","Epoch 10/100, Loss: 1.334049940109253\n","Epoch 11/100, Loss: 1.2235138416290283\n","Epoch 12/100, Loss: 1.2607295215129852\n","Epoch 13/100, Loss: 1.2078235447406769\n","Epoch 14/100, Loss: 1.1810083985328674\n","Epoch 15/100, Loss: 1.2362671196460724\n","Epoch 16/100, Loss: 1.2857175767421722\n","Epoch 17/100, Loss: 1.1699181497097015\n","Epoch 18/100, Loss: 1.2079658806324005\n","Epoch 19/100, Loss: 1.3222905099391937\n","Epoch 20/100, Loss: 1.2707453966140747\n","Epoch 21/100, Loss: 1.231934279203415\n","Epoch 22/100, Loss: 1.3081634044647217\n","Epoch 23/100, Loss: 1.18614062666893\n","Epoch 24/100, Loss: 1.288623183965683\n","Epoch 25/100, Loss: 1.24125736951828\n","Epoch 26/100, Loss: 1.149610549211502\n","Epoch 27/100, Loss: 1.1553161144256592\n","Epoch 28/100, Loss: 1.2186067998409271\n","Epoch 29/100, Loss: 1.098135620355606\n","Epoch 30/100, Loss: 1.1558214724063873\n","Epoch 31/100, Loss: 1.0487443655729294\n","Epoch 32/100, Loss: 1.207434356212616\n","Epoch 33/100, Loss: 1.0523445457220078\n","Epoch 34/100, Loss: 1.1357793509960175\n","Epoch 35/100, Loss: 1.0202729851007462\n","Epoch 36/100, Loss: 1.1360851228237152\n","Epoch 37/100, Loss: 1.0511284172534943\n","Epoch 38/100, Loss: 1.0238207131624222\n","Epoch 39/100, Loss: 0.9555166214704514\n","Epoch 40/100, Loss: 0.9957255125045776\n","Epoch 41/100, Loss: 0.9223334044218063\n","Epoch 42/100, Loss: 0.9028687179088593\n","Epoch 43/100, Loss: 1.0637447088956833\n","Epoch 44/100, Loss: 0.8949536085128784\n","Epoch 45/100, Loss: 0.9472440779209137\n","Epoch 46/100, Loss: 0.9152143448591232\n","Epoch 47/100, Loss: 0.9103216975927353\n","Epoch 48/100, Loss: 0.8871485441923141\n","Epoch 49/100, Loss: 1.0219225138425827\n","Epoch 50/100, Loss: 0.9904790669679642\n","Epoch 51/100, Loss: 0.8903968632221222\n","Epoch 52/100, Loss: 0.8577707707881927\n","Epoch 53/100, Loss: 0.9146649837493896\n","Epoch 54/100, Loss: 0.8295875936746597\n","Epoch 55/100, Loss: 0.9126558303833008\n","Epoch 56/100, Loss: 0.8098647743463516\n","Epoch 57/100, Loss: 1.0010309517383575\n","Epoch 58/100, Loss: 0.7887231558561325\n","Epoch 59/100, Loss: 0.8136176019906998\n","Epoch 60/100, Loss: 0.7739140689373016\n","Epoch 61/100, Loss: 0.8225323408842087\n","Epoch 62/100, Loss: 0.7496587485074997\n","Epoch 63/100, Loss: 0.7202658951282501\n","Epoch 64/100, Loss: 0.7154845297336578\n","Epoch 65/100, Loss: 0.7233249098062515\n","Epoch 66/100, Loss: 0.7361983954906464\n","Epoch 67/100, Loss: 0.7111926376819611\n","Epoch 68/100, Loss: 0.7208994179964066\n","Epoch 69/100, Loss: 0.7320044338703156\n","Epoch 70/100, Loss: 0.6660534590482712\n","Epoch 71/100, Loss: 0.7099574059247971\n","Epoch 72/100, Loss: 0.833428755402565\n","Epoch 73/100, Loss: 0.6954770982265472\n","Epoch 74/100, Loss: 0.647054634988308\n","Epoch 75/100, Loss: 0.691984012722969\n","Epoch 76/100, Loss: 0.6799086928367615\n","Epoch 77/100, Loss: 0.678383857011795\n","Epoch 78/100, Loss: 0.7553875297307968\n","Epoch 79/100, Loss: 0.6043301671743393\n","Epoch 80/100, Loss: 0.6992766559123993\n","Epoch 81/100, Loss: 0.6431031078100204\n","Epoch 82/100, Loss: 0.5762410759925842\n","Epoch 83/100, Loss: 0.6306722909212112\n","Epoch 84/100, Loss: 0.5783145427703857\n","Epoch 85/100, Loss: 0.6856432408094406\n","Epoch 86/100, Loss: 0.6340017765760422\n","Epoch 87/100, Loss: 0.5579198896884918\n","Epoch 88/100, Loss: 0.6904813647270203\n","Epoch 89/100, Loss: 0.5814138799905777\n","Epoch 90/100, Loss: 0.560299426317215\n","Epoch 91/100, Loss: 0.7080918252468109\n","Epoch 92/100, Loss: 0.5213531851768494\n","Epoch 93/100, Loss: 0.5472509115934372\n","Epoch 94/100, Loss: 0.6603541746735573\n","Epoch 95/100, Loss: 0.6207432299852371\n","Epoch 96/100, Loss: 0.5611091330647469\n","Epoch 97/100, Loss: 0.5817122161388397\n","Epoch 98/100, Loss: 0.5433818101882935\n","Epoch 99/100, Loss: 0.6321597844362259\n","Epoch 100/100, Loss: 0.5938495025038719\n"]}]},{"cell_type":"code","source":["def test_vqa_model(model, test_images, test_questions, test_answers):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i in range(len(test_images)):\n","            image = test_images[i].unsqueeze(0).to(device)\n","            question = model.bow.transform([test_questions[i]])\n","            question = torch.tensor(question.todense()).float().to(device)\n","            \n","            output = model(image, question)\n","            _, predicted = torch.max(output, 1)\n","            total += 1\n","            # print(test_answers[i], label_dict[test_answers[i]])\n","            # print(predicted)\n","            correct += sum(predicted == label_dict[test_answers[i]]).item()\n","\n","    accuracy = correct / total\n","    print(f'Accuracy: {accuracy * 100}%')\n","    return accuracy"],"metadata":{"id":"YchikApWyjCf","executionInfo":{"status":"ok","timestamp":1683576003378,"user_tz":240,"elapsed":369,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["train_images, train_qs, train_answers, test_images, test_qs, test_answers, answers = load_dataset()\n","test_dataloader = create_dataloader(test_images, test_qs, test_answers)\n","test_vqa_model(model, test_images, test_qs, test_answers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jq-huvAzjz2u","executionInfo":{"status":"ok","timestamp":1683576008386,"user_tz":240,"elapsed":766,"user":{"displayName":"Katie Liu","userId":"09945380277242665201"}},"outputId":"8017558a-4732-44fc-a105-4daa28a9b0ae"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 59.0%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.59"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":[],"metadata":{"id":"XeUR7f2cj0ep"},"execution_count":null,"outputs":[]}]}